We. Probably because it is cheaper.
Yeah, I does this.
But it doesn't.
She. Yeah.
But. But.
Ginger is doing well.
What is know is why did you go there this morning?
It's really. Any questions?
Any issues with your assignments. Solution to the first one is posted already in blackbirds.
Has anyone had a chance to reverse engineer the B-to-B algorithm and see what it what is.
What is it doing? No, I'll pass to it.
We'll do a walk through to offer an example.
I'll show you how it works for you. Clear. And then we'll talk about something that is called context free grammar.
Have you heard that term for some of you, Mike?
Okay, so let's go back a little bit.
Here's the idea. Here's something that we were working on discussing last time.
We did some preprocessing in our text invitational imposition.
Whatever we needed to do in our next step was let's have our text with part of speech.
Why would that be necessary? It will help us to determine the structure of of sentences.
It will help us to determine what are some of are actually legal within some syntax system,
or it might help us to correct sentences or discover some alternatives.
And finally, having a tagged text helps with upper level one level above processing tasks such as the named entity recognition or mapping.
He said that this or she did that, that mapping that read tacked sentences.
To make that happen, we need something that is called a tag, right part of speech tagger.
There are some several ways to build one using any python library that is modern.
For an LP,
you will have a deep learning model that you are talking about a hidden Markov model of language that helps us determined tags for our sentences.
So to build a model like that where it's a deep learning model he didn't like more or any other model, we need a corpus that is annotated with tax.
For every token in that corpus we have a tag associated with it.
Instead of just text. Based on that, we can build our.
Models like this. That's not the only way to build up a model representing some sequence generation.
In our case, it's a sequence of words generation.
But this is what do. So let me remind you that we never dealt with the lack of models or you're not too familiar.
There's several components to it. States was two states.
Not in our example here there is just one of them.
There is a special start state and there's an end state. And we're ignoring and states here and state here.
It's important for us to have that starts that we're using it to parse sentences that starts work.
States in our case will represent tax, but the market model can represent any and all sequence in time.
So it doesn't have to be dealing with text now.
We also have something called observations, which is which is the sequence, the output that we are actually seeing generated by the underlying system.
In our case, the language system with rules, grammar rules and whatnot.
So we need that. We have those two things.
We have states, tags, observations, words.
And to map the two we have.
Two tables of probabilities, transition probabilities that you will mature, describing how are states related to each other?
How likely is one state to follow Another likely is one part of the speech to follow another and then emission probabilities.
If I'm at a certain state, how likely is that state to emit something for me to observe an observation?
In our case, it won't be words so much as things.
Surely we have this. Those two tables at the bottom is the entire hidden Markov model.
This right here is just a graphical representation.
So to build a one. We need a tax corpus.
Right. This is actually a tax. But you can imagine that in the corpus you have words and tax associated with them.
Now, you can easily see here or you can easily deduce count.
Our estimate will be a better word than the Deuce. How likely is a noun to follow another?
Part of the speech. You're just looking at sequences. How likely is one to follow another?
This is information that can be extracted from a corpus. Also, you can extract how often.
Example the word where is tagged with the verb.
So. Tag sequences can be used to estimate what kind of probabilities for work you do.
Markov Model. Transition right now because we are transitioning from one to another, from one to another.
Right. And mission probability means how likely I have arrived at the verb state.
How likely am I to produce of where I work?
It doesn't have to be a work day.
And with that sort of model.
Here's an information.
If you've never looked at it, how would you actually use counts from the corpus to generate both the transition and emission probabilities tables?
Now once we have those and using the assumption, a bunch of assumptions, apparently.
Decided to use markups. Something like a single tack.
And its location depends on the previous owner, previous tax rate on the entire sequence of the previous tags.
We will use a Markov assumption to just cut it to.
Correct? That depends only on the previous day.
That's one mark of assumption. Same for a mission.
The mission of the word depends on the current tag only.
Even though if you were to capture a context, you would probably look at more territory altogether.
Here, have it yourself or not.
But we're dealing with a product of two conditional probabilities.
The one that you see here is the mission probability, and the other conditional probability with the transition probability.
Our goal is to find such a sequence of probabilities of a product that ends up the highest or a specific tag sequence.
Does that make sense? We already know the word sequence.
This is what we observe. Now, let me try to figure out what is the underlying sequence of text that produced that.
That's what I want to learn. You have multiple options.
Now which ones do we choose? So in a very simple example that I show you, last time we had only four possible attacks on the more.
Now, the test sentence that we used was flies like a flower to decide which which which tagging sequence is the best for it.
So the brute force approach is to go through every possible path in such a graph slices a verb,
and then followed by like which is a verb or not or preposition, right.
Or to call brute force it. That was the idea there.
And then calculate every possible permutation of that.
Which which come which probability, Which final probability comes to them, The maximum is our winner.
But that's not very efficient.
So this is where Viterbi algorithm comes in, which is similar to a minimum at a distance, integral to dynamic programing.
Approach based on the very simple idea that we if we're multiplying one thing
after another and with another with another to maximize that whole sequence,
it's in the works to pick the maximum at the first position, the second position, and so on.
All right. But let's let's get to it. So we have a little Markov model that represents all the rules of English.
Is it clear how that Markov model is being generated and how it's being used?
Two tables. Probabilities are estimated for both tables.
We have it. Some of them will be zero. Fine. Now, how are we decoding that?
Well, we're trying to find a sequence that maps states to.
So I will be using I will follow with the example.
I will follow whether it's a optional textbook.
Same terminology. They're actually using general way of describing observations as well.
Words as observations, general and states are going to be representing most tags are a states that will be using states and observations in.
But you can easily swap that to for this specific application.
Okay. We are given at the beginning.
We're given a sequence of words of length t that's involved in matter, something that the sequence of observations in time,
the first timestamp, second timestamp key timestamp g is the length of the sequence.
And also we have the, the model which are those two tables,
transition probabilities and the mission probably these are given table representing and possible underlying states tags.
This is given first thing that we need to do is create a probability matrix which looks like this.
The arrows representing states, columns representing a sequence of observation in time.
And then we have determine values at every position in the table.
This one starts in the lower left corner and good start in the upper.
Does it matter? But it starts with our start.
Start stage. Beginning of the sentence.
That makes sense. Do you remember what we did for a minimum distance?
We're populating a very similar table across from the state.
The right to do that over a certain cell, not state, really.
We will be doing a similar thing. But now observe that this column represents what?
Kind of the best of the best.
Absolutely. Well, all the algorithms tell that particular level which the most productive level as in first observation in sequence,
we're assigning values to two observations, assuming for every rote, assuming a different state that a different state generated this observation.
Let's see how much work state agent generated this observation.
Number State B Number. Tag A This word.
Number type B, This word number. Does that make sense? Okay.
And then follow trying to maximize the product.
Our symbol problem, which is four tags and four words for observations.
That table would look like that. Does that make sense? Article number preposition.
Yes. But is it. So is the amount of parts of speech.
The first rows of the columns, The number of rows is going to be the number of part of speech tags that you have.
The number call this number of observations. So if I had a longer sentence to analyze, I would have more columns.
If I had a broader set of tags, I would have more rows.
That's all right. That's and I don't really have a matrices.
This is the first number of the rows in this case it's going to be.
Columns and courtesies, Rose. You could. You could label any way you want.
But I think this is this makes everything clear. Great. All right.
So. This is our first step.
We're building this table. It's. It's empty right now. And here I'm going to do a little shenanigan with that people.
I will just spread out the cells because we will be moving from one cell to the other, as in transitioning from one cell to the other.
So that makes sense. So I'm leaving some space for arrows to illustrate what's going on at the bottom.
I have my hidden Markov model. I notice the notation.
There. Are those aa00 or a transition probability from state 02001 is probability of transitioning from state 0 to 1, and so on and so on.
Here we have something that is labeled as b I of old J, which means probability of state.
I am omitting observation.
J error is in parentheses is the observation B is that I'm sticking to the notation from here from your textbook, so I'll shoot this in.
All right. Well, we have numbers, we have words. Let's start working.
Our first step is to populate the first column.
This is where we start. Okay, We're looking at the first observations.
This is pretty easy. There's two steps to it. Calculate the beta p value and then find back pointers.
Remember about pointers from when you remember this. First they tell us how we found the solution.
Now how how can we retrace the path of getting better back Focus will be necessary.
So first column and this is again following the notation from your textbook.
This is just a bunch of. Same equation.
The Derby value for role one over one.
The probability of transitioning from the beginning, from the start state to the state we're looking at.
So. Well, one would be article.
What's the probability of transitioning from the start state?
You are an article that is the green, I'm sorry, the blue one.
And then once we transitioned, we are here.
What is the probability that this Arctic article they will omit the word flux emission probabilities, Blue transition, green mission.
Okay. Yes. We're not ready. Question.
So there's nothing fancy going on here.
Just populate. Calculate values, by the way.
I looked at whole of this example pretty hard, and I hope there is no calculation mistakes.
Something might have slipped in, but the solution is correct.
I think everything adds up, but it is not something, it's just a clerical error.
All the formulas are fine. All right.
So victory for our article implies probability of transitioning from start to article times probability of article generating omitting flows.
Does that make sense? Or different values.
I calculate them. What can you what can you observe here?
This is a toy example. Of course. So in real life, we will probably not have that many zeros.
Nice little. Corpus just way more data.
Does it make sense that some of those values are zero?
The grammar possibly does not allow a certain transition, so the probability of that transition is zero, right?
Then there is a fact that the word flys will never be tagged with a certain part of it.
That's just wrong. Right? So this is where those zeros may come come in from.
To transition is illegal or a mission is impossible.
Okay, so now we have our. Viterbi values for the first column, populate it.
Let's add some bad pointers. So all those moves that we made of those calculations were initial initialized in on the start start state.
So we're pointing back to start state. Nothing fancy, not just a stage number two.
Once we have the first following column copy, populate it, we will go through it now.
Three. Every observation or every time state step one after one and everyone will populate the corresponding column.
So it's a to the nested nested for loops.
Does that make sense? Let's start with t two.
So does it make sense that I highlighted one of those back pointers?
This is the most likely if we're trying to maximize because what we're after is a path through this matrix that maximizes the total product.
So these two are not going to help me maximize anything because they will reduce everything to zero.
This, on the other hand, is going to happen and it makes perfect sense because flies no flies should be tagged as a no one could be tagged as a verb,
but according to our model, that's that's not going to happen.
Does that make sense? Even though.
It's not impossible, though, in reality for flies to be tagged as a hazard in our little tiny problem, this deterrent value comes down to you.
So now our formula for every pore for the cell changes a little bit.
We'll be maximizing all be finding finding such a source in this previous column that maximizes the following product.
Whatever came before. Previous column. Source the the leading column times probability of transitioning from that cell
to the cell in question times probability of of of cell emitting the word life.
There will be four options.
Because let's say I want to calculate the value for this entire B cell, right?
I could have came just so from the top.
1.1.1. I have four options. So this determines previous state.
That should be. Yeah.
State. Or different paths.
This time. This time transition probability.
Two times a mission probability. This times this.
And transition probability times. Trends of emission probability. Okay.
Because of ability of.
Article being tax or the word like is zero.
Does that make sense? We are getting zeros all over here.
Yes, but shouldn't we just multiplying it to the maximum effect?
Well, the article that was.
Why are we even considering all of these? What if.
What if I have to? Let's.
Let's not talk about probability numbers. Just any numbers, Right.
What if I have four here? Right.
And one. Two here. Right. But I have one here and six here.
So four times six as as a product will maximize it's versus four.
Four times one, which will be four. So you cannot just simply take the max.
You just have to look at it as a whole. That question gets, I think, should you be using the brute force?
But we still have to compute every six.
Well, yes, but we're using the dynamic programing approach because I already have those values calculated for the next three cells right here.
I will be reusing reading, looking up those values that I'll have to recalculate.
In other words, I don't have to go through the tree branch again for that.
I already have that. Okay. Does that answer your question?
Okay. So zero. Let's keep going. Cycle on very low number, but still.
Are you saying that this approach is better because you're keeping the values first column Set up a brute force which says.
Just notice that we're here to to calculate.
This the value of this cell, Right. You and I would have to if I were to do it.
Brute force that I would have to go all the way to the start and calculate that path again.
But now I already have that. All right.
Maximum value. No.
Notice that previously for this one.
One. I'm sorry. Two, one cell. I have four pointers because I had a tie.
Right here. Every single value was zero. Shouldn't even put that point in there.
If. If. This leads to zero.
No matter what. You could actually use this card.
But my point is here, because this this path through 3 to 1 is not going to help us.
I think whoever travels to view V2 one is going to end up with a zero in the end, right?
So we might just ditch those back pointers.
But let me just be in sync with the pseudocode here of pseudocode.
Just says pick the back pointer that maximizes the whole thing 000 as a maximum of 0000.
So let's just put back pointers for every single one here.
In the end, it will not matter because we will not follow that path backwards.
So far, so good. Yes, I I'll like more efficient.
So like or I don't know, this is possible. Like if a box is equal to zero, you just that whole line.
Because here's what here's what you could do.
Right. So we're thinking about an array or a or a matrix, right?
Just fixed in size. We're just putting values in. But if you thought about very good question, if you thought about every column as a separate list,
write a list of nodes that you can get from the previous column to the next.
Okay. We found V2 one to be zero, which is a dead end.
Remove it from from the column to list. Right. And then we don't have a don't have a problem.
Sure. But that's, that's another level of of improving efficiency here.
And number two, I would not expect these to be zeros So frequently.
We're just we're working with a dataset.
We're working with a sample example where there is a lot of zero probabilities or that.
Lead to that. But I would not make that assumption, by the way, that the Derby approach is not only for a part of speech that flies like a flower.
This is a set of observations. This set of observations could be used, for example, Measurement The Time series.
Likely and tax states.
They can be moves. Left. Right.
Right. Anything. So he the Markov model plus disturbing and nice decoding strategy together we are we happen to use it for free art of speech that.
All right, let's keep on going. Okay.
Here's another one that has one solid winner.
We're done with this column. All the back pointers are shown.
Some of them make more sense than the other because they're not coming from zero.
So does that make sense? I mean, if you follow it, I'm not going to bore you with redoing the same thing.
Just make sure that you understand which numbers are being used, where exactly we'll end up with.
Another column populated.
And this one, the numbers are getting smaller and smaller.
But we're done with a populated the table.
Now, the termination step. Okay. The termination step is find the maximum cell value in the last column, which would be decimal.
Okay. And then. Find the back pointer for it.
Use the black pointer for it. Which leads me back to the three one.
All of this list the sequence of back pointers to get your solution.
Does that make sense? Does the sequence make sense?
Noun. Verb. Article. Noun. So population of the table is complex the quadratic.
And then going backwards, that's linear, right?
Doug? Yes, it's not a question, but it's a way that like, let's assume that in a case where the last column I do save lives,
it makes me sad because it's like, is this something that could possibly happen?
So that. Let's let's stick to our speech.
Can you imagine the same sentence being tagged in multiple ways?
Slightly different, but different ways, right? Sure.
That's two or three ways. Doesn't. It's not impossible now.
It's very unlikely. We we're dealing with such so many numbers to multiply, very unlikely that we will have the exact number for all three.
But still, it's possible. So you this is a design choice right now.
Am I picking the first one or am I grabbing all three?
Actually, this might become more than three because I could have passed backwards.
That's fork right here. But the same values, that's possible as well.
So that tagger might generate multiple sequences of tasks that it considers okay and viable targeting.
So it's a design choice. I'm giving the numbers a lot of multiplication that we're using here.
It's unlikely that you will be in that scenario,
but you could you could factor that and actually having multiple tag sequence ready when you move a step out from the attacker into parsing will help,
but we'll talk about it in a second. Great question. All right. Does that make sense?
Is it now clear how the derby works? Yes. I'm sorry.
I'm just a little bit confused about like the work that is being done because like earlier, it seemed like you were doing every single possibility.
Is that still the tactic? Right. Yes, I,
I consider it every every cell in the matrix I populate by going through all its predecessors every every time I populate a cell in in that matrix,
I'm always taking into account all its predecessors.
Okay, Max. Max. Out of these four.
And then I keep the value that is. That is maximal.
And if there's one maximum, I just put the one back pointer to where where was this Which, which previous cell left it at?
Maximal. The Derby algorithm is a clear AI works.
Beats the brute force, Of course.
Okay. Questions? Nope.
All right, so what can we do of this tagged sentence now?
So. I mean, remind you, we're doing things step by step.
Road text floats, it gets chopped, sentences them into tokens, lemon ties, whatever we have to do, then we're doing part of speech diagonals.
So we have our nice little input tax, but also with tax on top.
So the amount of interest also we could do the word and add to add additional relations between words we have, which is our cake of of information.
Just gross. We're getting more and more stuff to tap into and make some some decisions perhaps.
Okay, so what's the next step? We have a part of speech tags available.
Into the past. So what does the parser do in computer languages?
How many Java coders right here? Or C, C++.
What's going on? What? What does the compiler do about.
Compiles things, but it also does the parsing and it will never it will say, Oh, I'm not complaining that because.
There is a syntax error in your code right now.
Can you imagine a system processing English language that, Hey, give me an answer to this question.
Right? Here's the question. And then the machine comes like, Oh, this is an incorrect English, this is illegal sentence.
I cannot process that. So parsing, parsing, among other things, will tell You can tell you whether it's a legal sentence.
Sentence. No. Or remember one of the historical slides from the first week like so is Noam Chomsky was
trying to automate translation of an actual English language into some formal structure.
Formal language, formal languages. Right. This was his idea because of English language or other languages are to lose or to imprecise.
So that was his idea.
By the way, that word sense to some degree version would be where the word net comes in and
helps you decide which which meaning which which sense of the word is used.
But. Based on your experience with dealing with language, English language, texting, reading, tweets, or or comments on the Internet?
How often do you see something that is not grammatically correct?
A lot of times writing, but hopefully not.
But we always think. Can you understand?
Most of the time what someone met. You can write.
So do things here. Your pastor should not reject something illegal, perhaps, right?
Perhaps it should be. Well, the whole MLP application should be capable of handling something that is mildly not not grammatically correct.
Right. A lot of grammatical errors are perhaps corrected before.
But. That's an upper level of passing.
Let's do something basic first.
Now, fair warning. There's going to be some theory for for a little bit, which is necessary.
How many of you are dealt with? Computer science, grammar programing, language, grammars.
Thank. So the concept there will matter a lot.
Here is a context.
Three Language. What do you think of context?
Free language. The context free part means in a language.
Okay, hold it up. Our is English context for.
No, absolutely it's not. The surrounding matters.
Whatever came before, it matters. Context your language is, and I will give you a definition of a length of language.
From our computer science perspective in a second.
But a context free language is a language generated by so-called context free crap.
I'll give you a hint of what a language is. It's it's a it's a set of all possible sentences that that a grammar or a set of grammar.
Lou Rules can generate. Most likely it will be an infinite set, right?
We'll keep generating longer and longer sequences, more and more words and whatnot.
But those sequences are generated by a set of rules just like that Hidden Markov model
will only generate observations that follow the pattern that is inside of the box.
Transition, transition, transition. You cannot have. There's only a limited amount of possible transitions, right?
Similar concept here. What is a contact ST Grammar.
A system. A mathematical system for modeling constituent structure in languages.
I have a definition of a constituent is probably not.
Essentially a constituent is a grouping of words more than one word.
Phrases. Because remember those phrases.
Does that make sense? Understanding which subsequent supports in our sentence goes together.
There's a rule for that probably in the language that decides Verb follows a noun.
Blah, blah, blah. So a complex three grammar is a system that encodes those rules.
Essentially, if I see this, it can be interpreted or broken down as something else.
The grandma is made up of no rules.
Well, from now on, called Productions and a Lexicon, a set of language symbols can think of words, punctuation, signs, emojis, even.
But in English, mostly it will be mostly words. Okay.
This is a grammar. And pass the so called a start symbol.
And you can see some. Phrases.
Verb phrases. Noun phrases. Define production rules.
So what does this VP verb phrase section mean?
When I see a verb phrase? I can break it down.
It's legal for me to break it down into a single verb or a verb, followed by a noun phrase,
which is here, or a verb in a noun phrase, in a prepositional phrase, and so on and so on.
These are the legal chains in my system.
Note is that there are some recursive relationships, right?
The P verb and P go here. Right. This will actually end up as a short preposition, and it will end somewhere.
But production rules.
Does that make sense? The top one is a start symbol.
Okay. This means this is where this means us essentially a sentence.
What are the. Basic ways to break down a sentence.
In our simple grammar, we have an effort. We have a production of rules that says every sentence can be broken down into a number,
a sequence ordering matters, a sequence of noun and verb phrase.
Does that make sense? Production rule.
This can produce that, this or produce that or that or that, this can produce that or that.
And so on and so on. All these.
Elements right here are called non terminals in symbols.
They are representing. A stage in expansion.
I'll show you. An example, a lexicon means a sort of a mapping.
In this case, and now in our lexicon, a very simple lexicon, and now a non-trivial symbol.
Now, in here, anywhere you see a no no. Okay.
Down the line, it can be replaced by either one of these.
Of course, in English, that list is going to be way, way, way longer for possible terminal symbols.
We're expanding or expanding or expanding, and at the end, we have the actual word.
So far, so good. Okay. Non terminals, symbols that can be replaced by something else.
Terminal. This is the end of the word. The road that we cannot replace that with anything.
These are going to be typically just words.
So here's a couple examples how those expansions would work.
Given our lexicon. Questions.
Hopefully you will see in a second way why this all matters.
Okay, Let's go back to the context frame. Assumption expansion of a known terminal does not depend on its neighbors.
So here we have.
A sequence of verb, noun, praise and prepositional phrase.
Do I have to expand the noun phrase according to the grammar rules?
This is a non terminal, so our expansion does not end here.
I have to expect it now. And B can be expanded in three different ways.
Which one should I go for? Perhaps this verb dictates that perhaps this prepositional phrase dictates that perhaps
both of them dictate that perhaps I could go up a level and gather some context.
Right. To make to make the correct decision.
What should go here? What should go here? But I'm not doing that under the context free assumption.
I will just to the right. I see. And B, I expended.
I don't care what's on the left. On the right of it. I don't. I don't care.
I want to keep my life easy. Context free is not necessarily a super realistic assumption, but it's an assumption.
Does that remind you of a mark up assumption? To some degree.
We arrived at net B, I don't care how we erect that will go.
We're going to expand it. This is a very, very similar idea.
Okay. If given a start symbol or our beginning of o of our sentence expansion,
we can simply follow expansion rules or production rules that are here to build a past tree.
A past tree is. A representation of some sequences of words in the sentence are tokens and how are they related to each other?
Does this make sense? Grouping morning and flight.
It makes perfect sense, right? Now, is this the only possible past free for this?
I prefer a morning sentence. No, I have multiple options in my grandma.
Therefore, I can create multiple, multiple passwords.
What is what is now going back of 15 minutes?
What is helping me to build that tree? People at the table, but also the part of speech tags that we generate right now.
I have preferred label as a verb, so I can I can use that.
Okay, so here's a past treat before we get a little deeper.
Based on our experiences so far, but based on everything we've done so far,
if I give you two or three pass streets for a for a single sentence, how will you decide which one is best?
How would you do it? Yes, like we did that, David, beside the most.
The highest probability. Highest probability.
So, yes, we will be able to assign a probability to a tree and then pick the one that that has the highest probability.
Now, is this going to be the actual English language probability, tree probability for that sentence?
No, it's going to be some number relative to other numbers that will help us make a decision.
Same principle. Now, the formal formal definition of a context free grammar is.
It consists of a set up nocturnal system symbols which can consider variables.
And the noun phrase is a variable that can be replaced by something else.
That's a known terminal symbol. A set of terminal symbols, which would be would be that lexicon.
For the most part, they are as as a set of rules of production.
So these are. Production rules.
That allows us and this is very important, that allows us to move from one forum.
They play here to another forum.
Better. Production.
I have I have this form not fully expanded.
I can replace it with another form that that where I know that this is going to be in non terminals.
And also, for example, NTP would be a non terminal symbol on the left.
I can expand it into something else.
That's something else is better here and that is a string of symbols from the infinite set of strings that just groups, terminals and non terminals.
I can keep expanding and expanding and that could be the set of possible final expansions.
Expansions is infinite. That makes sense as designated as well.
What does it mean that I can derive?
One string from another. That means that there is a rule production rules that allows me to go from one form to another.
That means that this strain derives another strain.
That's another theoretical term. Given that strain, which is not necessarily the final sentence, but just a sequence of symbols.
Using my grammar, I can move to another world.
It's a little bit like propositional logic where you're there's an equivalence rule.
If I have this, I can turn it into that. I get to keep manipulating things until I get something else.
Direct derivation. There isn't not a direct derivation through a sequence of direct derivations as well.
If I have a production rule that turns this into that and that another production rule that turns off to help approve, blah, blah, blah, blah, blah.
Ultimately arriving at a sequence of symbols of M, that also means that through the sequence of productions I can derive.
I can derive, of course, the shooting new right here.
Alpha one arrives. Alpha. What doesn't?
What does it tell us? We're a part of our concern here is is testing legality of certain things.
I can go from one form to another through a bunch of productions.
That means this is a legal, legal commercial that in our grammar they cannot go from from the start symbol to my sentence, a bunch of manipulation.
That means that I that sentence is legal in my gravel.
I cannot claim that the s involved threw a bunch of productions into my final sentence.
This is all. I cannot make that connection. This is caught.
Is illegal in the ground. And I cannot also build a car street that corresponds to.
All right. Another form of definition language is a set of all strings composed of terminals and symbols that can be derived from s.
So a language is a set of all sentences.
That can be generated since Grandma.
Anything. Anything that cannot be any sequence, any string sequence that cannot be generated derived from the symbol is illegal in a given ground.
Forget about the notation. That basically means that if that sequence of words can not be produced by a sequence of productions,
that sequence of words or its death sentence is illegal according to several broad basis.
So if English was very, very rigid and everything, every sentence adhered to the rules of grammar.
If my rules of English grammar cannot lead me to generating that sentence, that sentences the legal language, period.
Okay, so the building of our street should be pretty simple is expand, expand, expand.
At the end, the leaf, the leaf symbols are going to be terminals, which are actual words.
And this process of starting with the symbol and then applying to rule, expand and build a power street is called syntactic parsing.
In the process because we have those rules, we have the ground, we are able to find the relationships between words.
We have our part of speech tags from the previous stage, right?
And now we have the we added the grammar. Put it all together, build it for our street.
And along the way we're finding what relates to what in that section, what is a subject, right?
We can find that kind of information to the person. Phrases and closes their relationships.
Does that make sense? So now that you know that a sentence can can have multiple, multiple parts or a three strike.
Essentially following a different sequence of production rules, different sequence of expansion.
How would you decide which what to expect next?
We talked about two part series having having different probabilities.
But this is this is after we build the first one, we go, what do we have to make a decision there as we go?
I mean, some of you were in my shoes for a very remember seems for for a couple of hours to show logic, the special form of a prepositional object.
This is a grammar for C and that form preposition.
You don't have to memorize it. This is just an example that you can express for language rules about arithmetic expressions.
Numbers digits. Smart operator.
So I send them a sentence in a medic ground would be.
Another sentence. Operator? Operator.
I'm sorry. Another sentence or just a number?
All right. A number can be in number eight, followed by digits are individual digit operators for you.
I'm sorry. For operators. Digits. These are the this is the lexicon right here.
This is the lexicon as well. Terminal symbols.
Rose. One plus two minus three.
If I have if I have a mathematics grandma like this, can I build the tree of our streets for that sentence?
How well is it? How do you think a calculator does that?
It passes your expression. Is it possible to generate an illegal expression or arithmetic expression?
Absolutely. Plus plus one, too. Right.
That's that's illegal. So there there will be no way using this grammar to build a path for it.
Reject it. Okay.
What about this? One plus two minus three.
Is it possible for me to get to our streets for the same mathematical expressions that are different?
Sure, it sure is. That means if you see something like that, if your if if you're capable of producing two park streets,
two or more park streets from the same sequence using the grammar rules, that means that your grammar is ambiguous.
It allows production of more than one street per sequence.
And this is where we have potential problems. There is no more thing to grammars are equivalent.
They can produce the same. Strings.
There's two levels to it. I was able to produce string a year from this drummer and a string from that grower.
That means that I don't care how, but I start to see the same strength, same strength.
That means weak equivalence. I know that they both can produce the same sequence of of strings, of the same string, sequence of symbols.
Now there is a strong equivalence if they will produce the same string using the same sequence of.
Their relations. Here's an example for you.
Two grammars that are equivalent to that. Does it matter for you a lot?
Probably not much. But it's good to understand how a grandmaster working in computer science to begin with.
Okay. So now let's say that we we have someone design a grammar for the whatever the language Greek our A.P. application is,
whatever our parsing parser is passing.
Let's say it's an English graph. Someone already designed a nice little set of rules that we have and.
Now we want to build trees. Our streets. How would we go about it?
What do we need? Or how would we even arrive at the of production?
Another type of corpus for you, someone to have to build that.
So someone had to build a corpus that goes beyond art is just it.
Also in that it included includes the pass stream information along with what even one level out there, this kind of caucuses as called a tree back.
And I know you have something and treatment is available for you.
This is what I was looking for, a group of groups of words that may behave as a single unit or phrase are called a constituent.
Passing means finding what is a constituent.
Essentially which groupings of words is should go together.
Examples. This should be pretty one thing you read.
The goal of again, linguistic wise is to distinguish between the independence.
I was sitting with. Starts with something and the rest is for the Beatles.
Pretty boring. All right.
So we already talked. I mentioned a bunch of times that if I if I can use the parser to decide whether the sentence is legal according to some group.
Yes, I'm given the sentence. Right. And if I'm able to build a grammar, if I'm able to build a tree base for that sentence using my grandma Jean,
that means that it's a legal sentence in background. So Grandma checking grandma early.
Does anyone use, Grandma? This is how you would use it.
Of course, you would use there is probably a deep learning network behind it, not.
Whatever you will see next. But that's the idea.
Am I able to build up our street or quality for whatever someone wrote?
If yes, illegal, legal. The legal sentence.
If not. I can suggest a legal mind, a close one, and a close sequence of words, that is, that I can actually build up our strength for.
Just. What?
Parsing happens in tasks like question answering who won the battle?
Blah, blah, blah blah blah. 1942 will be a question without past tree without the answer.
Ah. Did it not hold hands? The right.
First you will find the subject. For example one.
This will be something for you for the machine to latch onto are going backwards in generating
a response has to be a correct grammatically response using the same same named entity.
Your problems with parsing just do not and with just ambiguous.
Different trees for the same sentence. A sexually ambiguous ground.
There is a structural ambiguity.
There is attachment, ambiguity. If you can attach a part of a sentence to multiple places.
Coordination. I need you reading this. This one's important.
Is that all men and women is that you figure out what matters here.
What was the intention behind it? Context.
Context might help us. Yes, you can have a big sentences that can also be of weak equivalence, right?
Because this is like what?
We don't know how they were. I mean, you can assume, I guess I don't know.
So it could be we could have two grammars that can generate two two ambiguous trees for for me for that sentence.
You're. There's two approaches to do mechanical automated passing.
Bottom up and top down. Top down. We already talked about it a lot.
Start with the symbol and just try and balance.
Bottom up is let's start with the centers we already have as part of speech tags.
Let's try to match it to two roles.
I can have. I have three three words, a subset subset in my in my sentence.
Right? Three words. This is a noun. This is a verb. But can I find a matching role for that?
Yes, I found it. Okay, what level up Can I find a route that matches this and that and ultimately get to.
Yes. And if at some point of this bottom up passing, I end up with, oh, wait a minute, I don't have a rule for that.
That means that our sentence is. Illegal, according to some group in your friend Chomsky.
Again, many of you heard about Chomsky.
No law for. What is it?
I don't remember. That's.
Okay. Before we get into Chomsky's, not.
Not normal for water. Perhaps this pattern is not us.
This is history.
But can you. Have you seen certain performance increases with little tricks of different approaches so far, meaning memory distance?
Let's let's use the let's use the dynamic programing Viterbi.
So instead of brute force, let's be clever about it and try to cut down on processing.
Here's this working out of our string, you know, whether you're doing bottom up or top down.
This is going to require work. It's going to be.
Quite a few points where you have multiple options to consider.
So can we cut down on that work each?
All right. I'll leave you with that question. We'll come back to.
Just, you know, normal for my whatever follows because their algorithm taps into that.
I don't want to start talking about it. Well, companies Apple and yes,
the example with an Ace app is that you could and based on a discovery test
and that it's something you actually see what Chomsky's normal form is about.
You know, all the production rules in the grammar are of either one of those forms.
I have this, I can expand it into something else, but that's something else.
It's always going to be either two pieces or one unlike.
Oh, we have an example here. Really?
But. Oh, here you go.
There's a rule that that has that expands into three, three components or four or seven.
Which. All right, let's let's stop here.
I'll come back to it next time on Wednesday. Anything?
Any questions? Anything I need to slow to pass?
A few. If you want me to discard all this linguistic theory, I will not do that.
This is part of it. All right.
There's nothing else. Thank you so much for your support.
I think I have a marker. It's about the genetic algorithm now.
I think it was basically the question.
So when we get across to the grocery, we get the children to have one child because, well, yes, we should be hostages.
That has produce a a person when they take the first or is the first episode.
And I was going to say to that, stop, I'm trying to make the we count the expansion of starting.
The initial estimate expand.
You know, you expand. I don't want you to do that. The whole.
All right. Just one thing I definitely want to add in the carbon market rates while they were slightly lower.
And also, you know, I don't know here in the U.S. the way they did, I knew that was part of the assignment as well.
Even if most of the, you know, don't show time for the U.S. segment, I think we have to the Indians probably could be done today.
We learned that before. It was when we were talking about language models.
Using the word is in both the probabilities transition and the other one.
Oh, yeah. There's something like a matrix for that, as we know, and we just have to talk about all that conditional.
I thought it would be a sort of short answer, but I guess it's just it's just a product.
These products are above each part of speech with the sentence parts and all of them.
And I didn't understand the second question. How do you as a will in comedy, was it start giving the probably so and okay just.
It's giving some possibility. Okay. So how can you make it up with Hogan?
Relatable to. If it wasn't for them, I would have.
I. This is a person that is know how to generate a problem using this person.
So what does this mean? What does it mean? You have put a single word.
Okay. And when we have multiple ones, what percentage value?
How many being in the middle? How many? That's how many Arabs are in the country.
So we have total number of words which count? Total number of words we did and then count.
Ducks and ducks. That gives us point 28%.
So just convert it into the probability. Right. This is not probability.
Yeah. Just convert this percentage. What will be the probability by 1.1.
Where do you abandon. Yeah.
So 50% is 61.2. 8% is what it was.
50%. It would be 40.5 probability if it's .02800 to.
That should be easy. I know you have to be reminded of each word.
Let me start with you. Is it correct of.
Yes.
I think it's just don't don't confuse percentages, but with the ratios, they're representing the same thing but in a different way ratio percentages.
Same thing but in the. I think I continue to have on here.
So what, I'm trying to do this or this, but she's like, this is this is the first oh, this is the second second column buzzer.
So this tension probably shouldn't.
